{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":800230,"sourceType":"datasetVersion","datasetId":1305}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Loading Libraries","metadata":{}},{"cell_type":"code","source":"# Data handling\nimport pandas as pd\nimport numpy as np\n\n# Text processing\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\n\n# NLP advanced tasks\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom gensim import corpora, models\n\n# Sentiment analysis\nfrom textblob import TextBlob\n\n# Download NLTK resources\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:41:05.897402Z","iopub.execute_input":"2025-12-01T16:41:05.897594Z","iopub.status.idle":"2025-12-01T16:41:33.894113Z","shell.execute_reply.started":"2025-12-01T16:41:05.897575Z","shell.execute_reply":"2025-12-01T16:41:33.893497Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading Dataset and description","metadata":{}},{"cell_type":"code","source":"import bz2\nimport pandas as pd\n\n# Path to your file\nfile_path = \"/kaggle/input/amazonreviews/train.ft.txt.bz2\"\n\nreviews = []\nlabels = []\n\n# Open the bz2 compressed file\nwith bz2.open(file_path, mode='rt', encoding='latin-1') as f:\n    for line in f:\n        line = line.strip()\n        if line:\n            parts = line.split(' ', 1)  # split at first space\n            label = parts[0].replace(\"__label__\", \"\")\n            text = parts[1]\n            labels.append(int(label))\n            reviews.append(text)\n\n# Create DataFrame\ndf = pd.DataFrame({'review': reviews, 'rating': labels})\n\n# Quick overview\nprint(df.head())\nprint(\"\\nDataset Info:\")\nprint(df.info())\nprint(\"\\nMissing values per column:\")\nprint(df.isnull().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:41:54.693448Z","iopub.execute_input":"2025-12-01T16:41:54.693748Z","iopub.status.idle":"2025-12-01T16:43:18.539122Z","shell.execute_reply.started":"2025-12-01T16:41:54.693727Z","shell.execute_reply":"2025-12-01T16:43:18.538524Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exploring Reviews","metadata":{}},{"cell_type":"code","source":"# Add review length columns\ndf['review_length_chars'] = df['review'].apply(lambda x: len(str(x)))\ndf['review_length_words'] = df['review'].apply(lambda x: len(str(x).split()))\n\n# Plot review lengths\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(12,5))\n\n# Characters\nplt.subplot(1,2,1)\nsns.histplot(df['review_length_chars'], bins=50, color='skyblue')\nplt.title(\"Review Lengths (Characters)\")\nplt.xlabel(\"Number of Characters\")\n\n# Words\nplt.subplot(1,2,2)\nsns.histplot(df['review_length_words'], bins=50, color='salmon')\nplt.title(\"Review Lengths (Words)\")\nplt.xlabel(\"Number of Words\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:44:04.973975Z","iopub.execute_input":"2025-12-01T16:44:04.974276Z","iopub.status.idle":"2025-12-01T16:44:26.679065Z","shell.execute_reply.started":"2025-12-01T16:44:04.974254Z","shell.execute_reply":"2025-12-01T16:44:26.678520Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Basic Text cleaning","metadata":{}},{"cell_type":"code","source":"import re\nimport string\n\ndef clean_text(text):\n    text = str(text).lower()  # lowercase\n    text = re.sub(r'http\\S+', '', text)  # remove URLs\n    text = re.sub(r'[^a-z\\s]', '', text)  # remove punctuation & numbers\n    text = re.sub(r'\\s+', ' ', text).strip()  # remove extra spaces\n    return text\n\n# Apply cleaning\ndf['cleaned_review'] = df['review'].apply(clean_text)\n\n# Show before and after\nprint(df[['review', 'cleaned_review']].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:44:34.296601Z","iopub.execute_input":"2025-12-01T16:44:34.296894Z","iopub.status.idle":"2025-12-01T16:46:32.961983Z","shell.execute_reply.started":"2025-12-01T16:44:34.296871Z","shell.execute_reply":"2025-12-01T16:46:32.961227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.sample(n=50000, random_state=42).reset_index(drop=True)\nprint(\"Using subset:\", df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:46:38.884731Z","iopub.execute_input":"2025-12-01T16:46:38.885565Z","iopub.status.idle":"2025-12-01T16:46:39.373167Z","shell.execute_reply.started":"2025-12-01T16:46:38.885529Z","shell.execute_reply":"2025-12-01T16:46:39.372383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\ndef clean_text(text):\n    text = str(text).lower()\n    text = re.sub(r'http\\S+', '', text)\n    text = re.sub(r'[^a-z\\s]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\ndf['cleaned_review'] = df['review'].apply(clean_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:46:40.870740Z","iopub.execute_input":"2025-12-01T16:46:40.870983Z","iopub.status.idle":"2025-12-01T16:46:42.596572Z","shell.execute_reply.started":"2025-12-01T16:46:40.870966Z","shell.execute_reply":"2025-12-01T16:46:42.595984Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tokenization and Removing stopword","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom tqdm import tqdm\nimport nltk\n\nnltk.download('stopwords')\nnltk.download('wordnet')\n\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\ntqdm.pandas()\n\ndef fast_preprocess(text):\n    words = text.split()\n    words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words and len(w) > 2]\n    return words\n\ndf['tokens'] = df['cleaned_review'].progress_apply(fast_preprocess)\n\nprint(df[['cleaned_review', 'tokens']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:47:00.005401Z","iopub.execute_input":"2025-12-01T16:47:00.006110Z","iopub.status.idle":"2025-12-01T16:47:10.436098Z","shell.execute_reply.started":"2025-12-01T16:47:00.006086Z","shell.execute_reply":"2025-12-01T16:47:10.435359Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Word Clouds visualization","metadata":{}},{"cell_type":"code","source":"\nsample_text = \" \".join(df['cleaned_review'].sample(10000, random_state=42))\n\nwordcloud = WordCloud(width=800, height=400, background_color='white', max_words=200).generate(sample_text)\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title(\"Word Cloud of Amazon Reviews\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:47:44.933872Z","iopub.execute_input":"2025-12-01T16:47:44.934538Z","iopub.status.idle":"2025-12-01T16:47:47.589089Z","shell.execute_reply.started":"2025-12-01T16:47:44.934505Z","shell.execute_reply":"2025-12-01T16:47:47.588306Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fast sentiment analysis","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\n\ndef get_sentiment(text):\n    return TextBlob(text).sentiment.polarity\n\n# Apply only on sample for speed\ndf['sentiment'] = df['cleaned_review'].sample(20000).apply(get_sentiment)\n\n# Sentiment distribution plot\nimport seaborn as sns\nplt.figure(figsize=(6,4))\nsns.histplot(df['sentiment'].dropna(), bins=30)\nplt.title(\"Sentiment Polarity Distribution\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:47:51.703729Z","iopub.execute_input":"2025-12-01T16:47:51.704022Z","iopub.status.idle":"2025-12-01T16:47:59.620884Z","shell.execute_reply.started":"2025-12-01T16:47:51.704000Z","shell.execute_reply":"2025-12-01T16:47:59.620276Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## N grams analysis","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Use smaller subset for performance\nsample_reviews = df['cleaned_review'].sample(15000, random_state=42)\n\nvectorizer = CountVectorizer(ngram_range=(2,2), max_features=20)\nX_ngrams = vectorizer.fit_transform(sample_reviews)\n\n# Top 20 bigrams\nngrams = vectorizer.get_feature_names_out()\ncounts = X_ngrams.sum(axis=0).A1\n\nngram_freq = sorted(zip(ngrams, counts), key=lambda x: x[1], reverse=True)\n\nprint(\"Top 20 Bigrams:\")\nfor ngram, freq in ngram_freq:\n    print(ngram, \":\", freq)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:48:05.806578Z","iopub.execute_input":"2025-12-01T16:48:05.807301Z","iopub.status.idle":"2025-12-01T16:48:08.046588Z","shell.execute_reply.started":"2025-12-01T16:48:05.807280Z","shell.execute_reply":"2025-12-01T16:48:08.045940Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## TFIDF vector","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vectorizer = TfidfVectorizer(\n    max_features=5000,\n    ngram_range=(1,2),\n    stop_words='english'\n)\n\ntfidf_matrix = tfidf_vectorizer.fit_transform(\n    df['cleaned_review'].sample(20000, random_state=42)\n)\n\nprint(\"TF-IDF shape:\", tfidf_matrix.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:48:25.552005Z","iopub.execute_input":"2025-12-01T16:48:25.552591Z","iopub.status.idle":"2025-12-01T16:48:28.480905Z","shell.execute_reply.started":"2025-12-01T16:48:25.552566Z","shell.execute_reply":"2025-12-01T16:48:28.480272Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Topic modelling(LDA)","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import LatentDirichletAllocation\n\nlda_model = LatentDirichletAllocation(\n    n_components=5,\n    random_state=42,\n    max_iter=5,\n    n_jobs=-1\n)\n\nlda_model.fit(tfidf_matrix)\n\n# Display Topics\nfeature_names = tfidf_vectorizer.get_feature_names_out()\n\ndef display_topics(model, feature_names, n_top_words=8):\n    for idx, topic in enumerate(model.components_):\n        print(f\"\\nTopic {idx+1}:\")\n        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n\ndisplay_topics(lda_model, feature_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:48:32.078725Z","iopub.execute_input":"2025-12-01T16:48:32.079208Z","iopub.status.idle":"2025-12-01T16:48:49.453540Z","shell.execute_reply.started":"2025-12-01T16:48:32.079185Z","shell.execute_reply":"2025-12-01T16:48:49.452774Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Aspect extraction","metadata":{}},{"cell_type":"code","source":"aspects = [\"battery\", \"price\", \"quality\", \"delivery\", \"design\", \"performance\", \"service\", \"packaging\", \"screen\"]\n\ndef extract_aspects(text):\n    found = []\n    for asp in aspects:\n        if asp in text:\n            found.append(asp)\n    return found\n\n# Apply only on subset to avoid CPU overload\ndf['aspects'] = df['cleaned_review'].sample(20000, random_state=42).apply(extract_aspects)\n\ndf[['cleaned_review', 'aspects']].head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:49:36.817777Z","iopub.execute_input":"2025-12-01T16:49:36.818373Z","iopub.status.idle":"2025-12-01T16:49:36.930128Z","shell.execute_reply.started":"2025-12-01T16:49:36.818347Z","shell.execute_reply":"2025-12-01T16:49:36.929551Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Aspect based sentiment","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\n\ndef aspect_sentiment(text, aspects):\n    # Handle NaN or invalid aspect values\n    if aspects is None or isinstance(aspects, float) or aspects == []:\n        return None\n    \n    try:\n        return TextBlob(text).sentiment.polarity\n    except:\n        return None\n\ndf['aspect_sentiment'] = df.apply(\n    lambda x: aspect_sentiment(x['cleaned_review'], x['aspects']),\n    axis=1\n)\n\ndf[['cleaned_review', 'aspects', 'aspect_sentiment']].dropna().head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:50:23.854316Z","iopub.execute_input":"2025-12-01T16:50:23.855044Z","iopub.status.idle":"2025-12-01T16:50:25.775879Z","shell.execute_reply.started":"2025-12-01T16:50:23.855021Z","shell.execute_reply":"2025-12-01T16:50:25.775260Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Aspect frequency visaulization","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nimport matplotlib.pyplot as plt\n\naspect_list = []\nfor row in df['aspects'].dropna():\n    aspect_list.extend(row)\n\naspect_counts = Counter(aspect_list)\n\n# Bar plot\nplt.figure(figsize=(8,5))\nplt.bar(aspect_counts.keys(), aspect_counts.values())\nplt.xticks(rotation=45)\nplt.title(\"Aspect Frequency Distribution\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:50:33.289773Z","iopub.execute_input":"2025-12-01T16:50:33.290444Z","iopub.status.idle":"2025-12-01T16:50:33.450737Z","shell.execute_reply.started":"2025-12-01T16:50:33.290420Z","shell.execute_reply":"2025-12-01T16:50:33.450144Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Lightweight review summarizarion","metadata":{}},{"cell_type":"code","source":"def simple_summarizer(text, n_sentences=2):\n    sentences = text.split('.')\n    ranked = sorted(sentences, key=len, reverse=True)\n    return \". \".join(ranked[:n_sentences])\n\n# Apply only to small sample\ndf['summary'] = df['review'].sample(10000, random_state=42).apply(simple_summarizer)\n\ndf[['review', 'summary']].head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:50:36.906704Z","iopub.execute_input":"2025-12-01T16:50:36.907364Z","iopub.status.idle":"2025-12-01T16:50:36.949204Z","shell.execute_reply.started":"2025-12-01T16:50:36.907330Z","shell.execute_reply":"2025-12-01T16:50:36.948465Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Saving results ","metadata":{}},{"cell_type":"code","source":"export_df = df[['review', 'cleaned_review', 'tokens', 'sentiment', 'aspects', 'aspect_sentiment', 'summary']]\n\nexport_df.to_csv(\"amazon_nlp_project_results.csv\", index=False)\n\nprint(\"File saved as: amazon_nlp_project_results.csv (Ready for GitHub & Portfolio)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:50:41.754138Z","iopub.execute_input":"2025-12-01T16:50:41.754771Z","iopub.status.idle":"2025-12-01T16:50:43.857100Z","shell.execute_reply.started":"2025-12-01T16:50:41.754746Z","shell.execute_reply":"2025-12-01T16:50:43.856304Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Aspect wise sentiment summary","metadata":{}},{"cell_type":"code","source":"# Explode list of aspects so each aspect gets one row\naspect_df = df[['aspect_sentiment', 'aspects']].dropna().explode('aspects')\n\n# Calculate mean sentiment per aspect\naspect_summary = (\n    aspect_df\n    .groupby('aspects')['aspect_sentiment']\n    .mean()\n    .sort_values(ascending=False)\n    .reset_index()\n)\n\naspect_summary.head(10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:50:46.907972Z","iopub.execute_input":"2025-12-01T16:50:46.908540Z","iopub.status.idle":"2025-12-01T16:50:46.930275Z","shell.execute_reply.started":"2025-12-01T16:50:46.908509Z","shell.execute_reply":"2025-12-01T16:50:46.929713Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sentiment labelling","metadata":{}},{"cell_type":"code","source":"def label_sentiment(score):\n    if score > 0.1:\n        return \"Positive\"\n    elif score < -0.1:\n        return \"Negative\"\n    else:\n        return \"Neutral\"\n\naspect_summary[\"sentiment_label\"] = aspect_summary[\"aspect_sentiment\"].apply(label_sentiment)\n\naspect_summary.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:50:53.647901Z","iopub.execute_input":"2025-12-01T16:50:53.648188Z","iopub.status.idle":"2025-12-01T16:50:53.657794Z","shell.execute_reply.started":"2025-12-01T16:50:53.648166Z","shell.execute_reply":"2025-12-01T16:50:53.657185Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sentiment distributin per aspect","metadata":{}},{"cell_type":"code","source":"aspect_df[\"sentiment_label\"] = aspect_df[\"aspect_sentiment\"].apply(label_sentiment)\n\naspect_distribution = (\n    aspect_df.groupby(['aspects', 'sentiment_label'])\n    .size()\n    .unstack(fill_value=0)\n)\n\naspect_distribution.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:50:56.184931Z","iopub.execute_input":"2025-12-01T16:50:56.185206Z","iopub.status.idle":"2025-12-01T16:50:56.210232Z","shell.execute_reply.started":"2025-12-01T16:50:56.185183Z","shell.execute_reply":"2025-12-01T16:50:56.209526Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Aspect importance score","metadata":{}},{"cell_type":"code","source":"# Explode aspects for counting\naspect_freq = df[['aspects']].dropna().explode('aspects')\n\n# Count frequency of each aspect\naspect_frequency = aspect_freq['aspects'].value_counts().reset_index()\naspect_frequency.columns = ['aspect', 'frequency']\n\naspect_frequency.head(10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:50:58.447290Z","iopub.execute_input":"2025-12-01T16:50:58.447994Z","iopub.status.idle":"2025-12-01T16:50:58.471208Z","shell.execute_reply.started":"2025-12-01T16:50:58.447968Z","shell.execute_reply":"2025-12-01T16:50:58.470610Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Merge frequency with sentiment summary\naspect_importance = aspect_summary.merge(\n    aspect_frequency,\n    left_on=\"aspects\",\n    right_on=\"aspect\"\n)\n\n# Compute importance score\naspect_importance[\"importance_score\"] = (\n    aspect_importance[\"frequency\"] * aspect_importance[\"aspect_sentiment\"].abs()\n)\n\n# Sort by importance\naspect_importance = aspect_importance.sort_values(\n    by=\"importance_score\", ascending=False\n).reset_index(drop=True)\n\naspect_importance[['aspects', 'frequency', 'aspect_sentiment', 'importance_score']].head(10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:51:01.399291Z","iopub.execute_input":"2025-12-01T16:51:01.399629Z","iopub.status.idle":"2025-12-01T16:51:01.417593Z","shell.execute_reply.started":"2025-12-01T16:51:01.399606Z","shell.execute_reply":"2025-12-01T16:51:01.416928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"top_important = aspect_importance.head(15)\n\nplt.figure(figsize=(10,6))\nplt.bar(top_important['aspects'], top_important['importance_score'])\nplt.xticks(rotation=45, ha='right')\nplt.title(\"Top 15 Most Important Aspects (By Impact)\")\nplt.xlabel(\"Aspect\")\nplt.ylabel(\"Importance Score\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:51:08.184431Z","iopub.execute_input":"2025-12-01T16:51:08.184738Z","iopub.status.idle":"2025-12-01T16:51:08.380148Z","shell.execute_reply.started":"2025-12-01T16:51:08.184720Z","shell.execute_reply":"2025-12-01T16:51:08.379541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"aspect_importance.to_csv(\"aspect_importance_scores.csv\", index=False)\nprint(\"Saved: aspect_importance_scores.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:51:11.429675Z","iopub.execute_input":"2025-12-01T16:51:11.429929Z","iopub.status.idle":"2025-12-01T16:51:11.435329Z","shell.execute_reply.started":"2025-12-01T16:51:11.429913Z","shell.execute_reply":"2025-12-01T16:51:11.434701Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa","metadata":{}},{"cell_type":"code","source":"# load BART summarizer\nfrom transformers import pipeline\n\n# Load BART summarizer\nbart_summarizer = pipeline(\n    \"summarization\",\n    model=\"facebook/bart-large-cnn\",\n    device=0  # uses GPU if available\n)\n\nprint(\"BART summarizer loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:56:24.289597Z","iopub.execute_input":"2025-12-01T16:56:24.290161Z","iopub.status.idle":"2025-12-01T16:57:08.236645Z","shell.execute_reply.started":"2025-12-01T16:56:24.290139Z","shell.execute_reply":"2025-12-01T16:57:08.235888Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Summarize sample reviews using BART\n# Select a few long reviews for summarization\nsample_reviews = df['review'].dropna().sample(3, random_state=42).tolist()\n\nbart_summaries = []\n\nfor text in sample_reviews:\n    summary = bart_summarizer(\n        text,\n        max_length=70,\n        min_length=25,\n        do_sample=False\n    )[0]['summary_text']\n    \n    bart_summaries.append((text, summary))\n\n# Show results\nfor original, summary in bart_summaries:\n    print(\" ORIGINAL REVIEW \")\n    print(original[:500], \"...\")\n    print(\"\\n BART SUMMARY\")\n    print(summary)\n    print(\"\\n\" + \"=\"*80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:57:50.114066Z","iopub.execute_input":"2025-12-01T16:57:50.115053Z","iopub.status.idle":"2025-12-01T16:57:53.751893Z","shell.execute_reply.started":"2025-12-01T16:57:50.115026Z","shell.execute_reply":"2025-12-01T16:57:53.751066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load T5 summarizer\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\nt5_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\nt5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(\"cuda\")\n\nprint(\"T5 summarizer loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:58:15.798568Z","iopub.execute_input":"2025-12-01T16:58:15.799339Z","iopub.status.idle":"2025-12-01T16:58:20.062856Z","shell.execute_reply.started":"2025-12-01T16:58:15.799311Z","shell.execute_reply":"2025-12-01T16:58:20.062158Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Summarize reviews using T5\ndef t5_summarize(text):\n    input_text = \"summarize: \" + text\n    inputs = t5_tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(\"cuda\")\n\n    summary_ids = t5_model.generate(\n        inputs,\n        max_length=70,\n        min_length=25,\n        length_penalty=2.0,\n        num_beams=4,\n        early_stopping=True\n    )\n    return t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\nt5_summaries = []\n\nfor text in sample_reviews:\n    summary = t5_summarize(text)\n    t5_summaries.append((text, summary))\n\n# Display T5 summaries\nfor original, summary in t5_summaries:\n    print(\" ORIGINAL REVIEW\")\n    print(original[:500], \"...\")\n    print(\" T5 SUMMARY\")\n    print(summary)\n    print(\"\\n\" + \"=\"*80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:58:59.674693Z","iopub.execute_input":"2025-12-01T16:58:59.675409Z","iopub.status.idle":"2025-12-01T16:59:02.521111Z","shell.execute_reply.started":"2025-12-01T16:58:59.675385Z","shell.execute_reply":"2025-12-01T16:59:02.520422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save summaries for Github\nsummary_df = pd.DataFrame({\n    \"original_review\": [x[0] for x in bart_summaries],\n    \"bart_summary\": [x[1] for x in bart_summaries],\n    \"t5_summary\": [y[1] for y in t5_summaries]\n})\n\nsummary_df.to_csv(\"review_summarization_samples.csv\", index=False)\n\nprint(\"Saved: review_summarization_samples.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:59:56.375541Z","iopub.execute_input":"2025-12-01T16:59:56.376147Z","iopub.status.idle":"2025-12-01T16:59:56.383353Z","shell.execute_reply.started":"2025-12-01T16:59:56.376124Z","shell.execute_reply":"2025-12-01T16:59:56.382598Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save outputs for github","metadata":{}},{"cell_type":"code","source":"aspect_summary.to_csv(\"aspect_sentiment_summary.csv\", index=False)\naspect_distribution.to_csv(\"aspect_sentiment_distribution.csv\")\n\ndf[['cleaned_review', 'aspects', 'aspect_sentiment']].to_csv(\n    \"review_aspect_sentiment.csv\", index=False\n)\n\nprint(\"All result files saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:51:15.775876Z","iopub.execute_input":"2025-12-01T16:51:15.776138Z","iopub.status.idle":"2025-12-01T16:51:16.396818Z","shell.execute_reply.started":"2025-12-01T16:51:15.776118Z","shell.execute_reply":"2025-12-01T16:51:16.396173Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualization github ready","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntop_aspects = aspect_summary.head(15)\n\nplt.figure(figsize=(10,5))\nplt.bar(top_aspects[\"aspects\"], top_aspects[\"aspect_sentiment\"])\nplt.xticks(rotation=45)\nplt.title(\"Top 15 Aspects by Average Sentiment\")\nplt.xlabel(\"Aspect\")\nplt.ylabel(\"Sentiment Score\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:51:19.359977Z","iopub.execute_input":"2025-12-01T16:51:19.360552Z","iopub.status.idle":"2025-12-01T16:51:19.554440Z","shell.execute_reply.started":"2025-12-01T16:51:19.360524Z","shell.execute_reply":"2025-12-01T16:51:19.553845Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Summary ","metadata":{}},{"cell_type":"code","source":"final_table = aspect_summary.merge(\n    aspect_distribution, on=\"aspects\"\n)\n\nfinal_table.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:51:23.639027Z","iopub.execute_input":"2025-12-01T16:51:23.639589Z","iopub.status.idle":"2025-12-01T16:51:23.649820Z","shell.execute_reply.started":"2025-12-01T16:51:23.639565Z","shell.execute_reply":"2025-12-01T16:51:23.649254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"top_positive = aspect_summary.sort_values(by=\"aspect_sentiment\", ascending=False).head(5)\ntop_negative = aspect_summary.sort_values(by=\"aspect_sentiment\").head(5)\n\nprint(\"Top 5 Positive Aspects:\")\nprint(top_positive)\n\nprint(\"\\nTop 5 Negative Aspects:\")\nprint(top_negative)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:51:25.433831Z","iopub.execute_input":"2025-12-01T16:51:25.434552Z","iopub.status.idle":"2025-12-01T16:51:25.443407Z","shell.execute_reply.started":"2025-12-01T16:51:25.434525Z","shell.execute_reply":"2025-12-01T16:51:25.442432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import zipfile\nimport os\n\n# Create a list of all files you want to include in the final ZIP\nproject_files = [\n    # Main processed data\n    \"amazon_nlp_project_results.csv\",\n    \n    # Aspect sentiment outputs\n    \"aspect_sentiment_summary.csv\",\n    \"aspect_sentiment_distribution.csv\",\n    \"review_aspect_sentiment.csv\",\n    \n    # Aspect importance scores\n    \"aspect_importance_scores.csv\",\n    \n    # Summarization results (BART & T5)\n    \"review_summarization_samples.csv\",\n    \n    # Topic modeling (if saved earlier)\n    # Add here if you exported any topic files\n]\n\n# ---- OPTIONAL ----\n# Automatically include any PNG/JPG plots you saved\nfor f in os.listdir(\".\"):\n    if f.endswith(\".png\") or f.endswith(\".jpg\"):\n        project_files.append(f)\n\n# Name of final ZIP\nzip_filename = \"amazon_reviews_nlp_full_project.zip\"\n\n# Create ZIP\nwith zipfile.ZipFile(zip_filename, \"w\") as zipf:\n    for file in project_files:\n        if os.path.exists(file):\n            zipf.write(file)\n            print(f\"Added: {file}\")\n        else:\n            print(f\"Missing or not found (skipped): {file}\")\n\nprint(\"\\nðŸŽ‰ ZIP CREATED SUCCESSFULLY:\", zip_filename)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:02:19.788171Z","iopub.execute_input":"2025-12-01T17:02:19.788920Z","iopub.status.idle":"2025-12-01T17:02:19.973891Z","shell.execute_reply.started":"2025-12-01T17:02:19.788897Z","shell.execute_reply":"2025-12-01T17:02:19.973281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}